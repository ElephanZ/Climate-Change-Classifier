{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab0addc2",
   "metadata": {},
   "source": [
    "<p style=\"text-align: center; font-size: 35px;\"><b>Social Media Management</b></p>\n",
    "<p style=\"margin-top: -20px; text-align: center; font-size: 27px;\"><b>Final Project</b></p>\n",
    "<p style=\"margin-top: -10px; text-align: center; font-size: 18px;\"><b>Classifier for Climate Change Tweets</b></p>\n",
    "<p style=\"text-align: center; font-size: 16px;\"><a href=\"https://antonioscardace.altervista.org/\">Antonio Scardace</a> • 1000007272 • 2021/2022</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c5e6bf",
   "metadata": {},
   "source": [
    "## Introduction to Project Idea and Data\n",
    "This task consists, given a series of tweets about Climate Change, in figuring out whether the user (the author) is skeptic or supports the belief of man-made climate change. Once implemented, trained, and tested, this algorithm will be useful in some real contests.\n",
    "\n",
    "To solve this problem, I have made an algorithm (a classifier) which has been trained on a **43943 tweets** dataset collected between 2015-04-27 and 2018-02-21 by ***Canada Foundation for Innovation JELF Grant to Chris Bauch, University of Waterloo***. <br/>\n",
    "Each row of the dataset contains: the text of the tweet labelled as '*message*', the tweet id labelled as '*tweetid*', and the sentiment of the tweet labelled as '*sentiment*'.\n",
    "\n",
    "Each sentiment is labelled as one of the following classes:\n",
    "- ``-1`` (**Anti**) &#8594; the tweet author doesn't believe in man-made climate change;\n",
    "- ``0`` (**Neutral**) &#8594; the tweet author neither supports nor refutes the belief of man-made climate change;\n",
    "- ``1`` (**Pro**) &#8594; the tweet author supports the belief of man-made climate change;\n",
    "- ``2`` (**News**) &#8594; the tweet links to factual news about climate change;\n",
    "\n",
    "<img src=\"https://antonioscardace.altervista.org/smm/dataset_distr.png\" alt=\"dataset tweets distribution\" style=\"width: 550px; margin-top: 10px; border: 1px solid #555\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12e0bf6",
   "metadata": {},
   "source": [
    "## Base Code\n",
    "\n",
    "I had to import all the modules I needed:\n",
    "- ``requests`` allows to send HTTP requests in a very easy way.\n",
    "- ``numpy`` adds support for big arrays and matrices, along with a large collection of math functions.\n",
    "- ``pandas`` helps with data manipulation and analysis through charts and tables.\n",
    "- ``re`` provides Regular Expression matching operations.\n",
    "- ``vaderSentiment`` is a lexicon and rule-based sentiment-analysis tool which is sensitive to web-based media texts.\n",
    "- ``sklearn`` is a library for Machine Learning on Python.\n",
    "\n",
    "Through ``sklearn`` I have imported some methods and classes:\n",
    "- **train_test_split** _(method)_ &#8594; it splits dataset into Training Set and Test Set.\n",
    "- **KNeighborsClassifier** _(class)_ &#8594; implements the K-Nearest Neighbors Classifier.\n",
    "- **MultinomialNB** _(class)_ &#8594; implements the Multinomial Naive Bayes Classifier.\n",
    "- **LogisticRegression** _(class)_ &#8594; implements the Logistic Regression Classifier.\n",
    "- **SGDClassifier** _(class)_ &#8594; implements Linear Classifiers (such as SVM, Logistic Regression) with SGD.\n",
    "- **SVC** _(class)_ &#8594; implements SVC (Support Vector Classifier) which is an implementation of SVM (Support Vector Machine).\n",
    "- **GridSearchCV** _(class)_ &#8594; it provides methods to optimize parameters into the algorithm (such as K in KNN algorithm).\n",
    "- **ShuffleSplit** _(class)_ &#8594; for large datasets it is used to improve the work of GridSearchCV bypassing the Cross-Validation.\n",
    "- **CountVectorizer** _(class)_ &#8594; converts a collection of text documents to a matrix of token counts.\n",
    "- **TfidfTransformer** _(class)_ &#8594; transforms a count matrix to a normalized tf or tf-idf representation.\n",
    "- **Pipeline** _(class)_ &#8594; sequentially applies a list of operations and a final estimator (classifier, in our case). Implements methods to train and test our algorithm.\n",
    "- **f1_score** _(method)_ &#8594; F1-score combines the precision and recall of a classifier into a single metric by taking their harmonic mean (one for each output class). \n",
    "- **accuracy_score** _(method)_ &#8594; Accuracy is one metric for evaluating classification models. Informally, it is the fraction of predictions our model got right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60bc4fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import re\n",
    "import vaderSentiment.vaderSentiment\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8360c216",
   "metadata": {},
   "source": [
    "Additionally, I have created two classes to make a good code (readble and reliable).\n",
    "\n",
    "**TwitterAPI** &#8594; Implements methods for connecting, authenticating, and retrieving tweets and user data. All via REST APIs.\n",
    "- ``getTweetsByUserId(id)`` &#8594; Gets user tweets by its id. We get just _nmax_ tweets per call.\n",
    "- ``getTweetById(id)`` &#8594; Gets tweet info by its id.\n",
    "- ``getUserByHandle(handle)`` &#8594; Gets user info by its handle (e.g. @JohnDoe).\n",
    "- I had to set also a **Bearer Token** attribute, which is a string and is the predominant type of access token used with **OAuth 2.0**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e168ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwitterAPI:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.__BEARER = 'AAAAAAAAAAAAAAAAAAAAAAYXVQEAAAAADCquAxAdP8tVr%2BGc7OvHAnlgans%3DbGrWjD5c31ld2CTvTTM0nhyXBYEtTH1oBhqoAkM8ycPhv9Lfy2'\n",
    "    \n",
    "    def getTweetsByUserId(self, id, nmax):\n",
    "        headers = { 'Authorization': \"Bearer \" + self.__BEARER }\n",
    "        response = requests.get(f'https://api.twitter.com/2/users/{id}/tweets?max_results={nmax}', headers=headers)\n",
    "        return response.json()\n",
    "\n",
    "    def getTweetById(self, id):\n",
    "        headers = { 'Authorization': \"Bearer \" + self.__BEARER }\n",
    "        response = requests.get(f'https://api.twitter.com/2/tweets?ids={id}&expansions=author_id', headers=headers)\n",
    "        return response.json()\n",
    "\n",
    "    def getUserByHandle(self, handle):\n",
    "        handle = handle.replace('@', '')\n",
    "        headers = { 'Authorization': \"Bearer \" + self.__BEARER }\n",
    "        response = requests.get(f'https://api.twitter.com/2/users/by/username/{handle}?user.fields=verified', headers=headers)\n",
    "        return response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195ba7a4",
   "metadata": {},
   "source": [
    "**AnalyzeVIP** &#8594; Implements methods to retrieve the tweets of a VIP, classify them, and transform the predictions into a DataFrame.\n",
    "- ``__init__(handle)`` &#8594; Gets user id by its handle.\n",
    "- ``__loadTweets()`` &#8594; Gets last tweets of the user.\n",
    "- ``__classify(c)`` &#8594; Puts tweets texts into the classifier to get predictions.\n",
    "- ``makeTable(c)`` &#8594; Makes a DataFrame table with two columns: _message_ and _predict_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b6bb1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnalyzeVIP:\n",
    "\n",
    "    def __init__(self, handle):\n",
    "        self.__twitter = TwitterAPI()\n",
    "        self.__id = self.__twitter.getUserByHandle(handle)['data']['id']\n",
    "\n",
    "    def __loadTweets(self, rows):\n",
    "        tweets = self.__twitter.getTweetsByUserId(self.__id, rows)\n",
    "        self.__texts = [tweet['text'] for tweet in tweets['data']]\n",
    "\n",
    "    def __classify(self, c, rows):\n",
    "        self.__loadTweets(rows)\n",
    "        return c.predict(self.__texts)\n",
    "\n",
    "    def makeTable(self, c, rows):\n",
    "        preds = self.__classify(c, rows)\n",
    "        dict = {'message': self.__texts, 'predict': preds}\n",
    "        return pd.DataFrame(data=dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d4b68c",
   "metadata": {},
   "source": [
    "I have also implemented this **preprocessing** function ``preProc`` using **Regular Expressions** (RegEx). <br/>\n",
    "It is a method useful to remove links inside the tweets text, because they confuse the classifier in multiple ways. <br/>\n",
    "For instance: in the dataset there may be a tweet (sentiment **J**) containing just an image (represented by a link). Since there is no text to analyze, the classifier will associate (a priori) each time a tweet consisting of only a link appears, the output class **J** (it depends just on luck)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d4e0c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preProc(doc):\n",
    "    return re.sub(r\"(?:\\@|https?\\://)\\S+\", \"\", doc).replace('RT ', '')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12fd61c2",
   "metadata": {},
   "source": [
    "## Dataset Loading\n",
    "\n",
    "As first thing, I have needed to load the dataset from a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e61f0a9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>@tiniebeany climate change is an interesting h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>RT @NatGeoChannel: Watch #BeforeTheFlood right...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Fabulous! Leonardo #DiCaprio's film on #climat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>RT @Mick_Fanning: Just watched this amazing do...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>RT @cnalive: Pranita Biswasi, a Lutheran from ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                            message\n",
       "0         -1  @tiniebeany climate change is an interesting h...\n",
       "1          1  RT @NatGeoChannel: Watch #BeforeTheFlood right...\n",
       "2          1  Fabulous! Leonardo #DiCaprio's film on #climat...\n",
       "3          1  RT @Mick_Fanning: Just watched this amazing do...\n",
       "4          2  RT @cnalive: Pranita Biswasi, a Lutheran from ..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets = pd.read_csv('dataset.csv')\n",
    "tweets.pop('tweetid')\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb6edad",
   "metadata": {},
   "source": [
    "## Experimental Dataset\n",
    "\n",
    "**As a test** I have extracted and added as features also the sentiment of each tweet thanks to **VADER**. <br/>\n",
    "Each sentiment returned from VADER is composed by **four scores**: negative, neutral, positive, and compound (is a normalized metric)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3faa53b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>message</th>\n",
       "      <th>neg</th>\n",
       "      <th>neu</th>\n",
       "      <th>pos</th>\n",
       "      <th>com</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>@tiniebeany climate change is an interesting h...</td>\n",
       "      <td>8.1</td>\n",
       "      <td>62.2</td>\n",
       "      <td>29.7</td>\n",
       "      <td>64.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>RT @NatGeoChannel: Watch #BeforeTheFlood right...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Fabulous! Leonardo #DiCaprio's film on #climat...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>54.4</td>\n",
       "      <td>45.6</td>\n",
       "      <td>85.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>RT @Mick_Fanning: Just watched this amazing do...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>74.3</td>\n",
       "      <td>25.7</td>\n",
       "      <td>67.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>RT @cnalive: Pranita Biswasi, a Lutheran from ...</td>\n",
       "      <td>15.6</td>\n",
       "      <td>73.6</td>\n",
       "      <td>10.8</td>\n",
       "      <td>-27.32</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                            message   neg    neu  \\\n",
       "0         -1  @tiniebeany climate change is an interesting h...   8.1   62.2   \n",
       "1          1  RT @NatGeoChannel: Watch #BeforeTheFlood right...   0.0  100.0   \n",
       "2          1  Fabulous! Leonardo #DiCaprio's film on #climat...   0.0   54.4   \n",
       "3          1  RT @Mick_Fanning: Just watched this amazing do...   0.0   74.3   \n",
       "4          2  RT @cnalive: Pranita Biswasi, a Lutheran from ...  15.6   73.6   \n",
       "\n",
       "    pos    com  \n",
       "0  29.7  64.28  \n",
       "1   0.0   0.00  \n",
       "2  45.6  85.44  \n",
       "3  25.7  67.05  \n",
       "4  10.8 -27.32  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vader = vaderSentiment.vaderSentiment.SentimentIntensityAnalyzer()\n",
    "\n",
    "tweets['neg'], tweets['neu'], tweets['pos'], tweets['com'] = 0, 0, 0, 0\n",
    "idx = 0\n",
    "\n",
    "for tweet in tweets.iterrows():\n",
    "    sentiment = vader.polarity_scores(preProc(tweet[1]['message']))\n",
    "\n",
    "    tweets.loc[idx, 'neg'] = sentiment['neg'] * 100\n",
    "    tweets.loc[idx, 'neu'] = sentiment['neu'] * 100\n",
    "    tweets.loc[idx, 'pos'] = sentiment['pos'] * 100\n",
    "    tweets.loc[idx, 'com'] = sentiment['compound'] * 100\n",
    "\n",
    "    idx += 1\n",
    "\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c5a5ca3",
   "metadata": {},
   "source": [
    "I have tested this dataset with each pipeline and each algorithm. Here is an example with the pipeline of KNN algorithm. <br/>\n",
    "Unfortunately, as you can see below, this new version of the dataset did not bring better results to the model. On the contrary, it made them worse than the KNN algorithm that you can find [below in the project](#comparison-of-classification-algorithms). <br/>\n",
    "So, in addition to this, I have obtained just a more complex model: bigger space and time complexity. For these reasons I didn't use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32f550f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin # they allow us to extend fit and transform methods by our features\n",
    "from sklearn.pipeline import FeatureUnion # allows us to work with multiple features\n",
    "from sklearn.preprocessing import MaxAbsScaler # normalizes features in a range [0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984959dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NumberSelector(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return X[[self.key]]\n",
    "\n",
    "class TextSelector(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return X[self.key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90fca49",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_exp = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "        ('text', Pipeline([\n",
    "            ('selector', TextSelector(key='message')),\n",
    "            ('vect', CountVectorizer(stop_words=None, strip_accents=None, lowercase=True, max_df=0.75, preprocessor=preProc)),\n",
    "            ('tfidf', TfidfTransformer(use_idf=False)),\n",
    "            ('standard', MaxAbsScaler())\n",
    "        ])),\n",
    "        ('pos', Pipeline([\n",
    "            ('selector', NumberSelector(key='pos')),\n",
    "            ('standard', MaxAbsScaler())\n",
    "        ])),\n",
    "        ('neg', Pipeline([\n",
    "            ('selector', NumberSelector(key='neg')),\n",
    "            ('standard', MaxAbsScaler())\n",
    "        ])),\n",
    "        ('neu', Pipeline([\n",
    "            ('selector', NumberSelector(key='neu')),\n",
    "            ('standard', MaxAbsScaler())\n",
    "        ])),\n",
    "        ('com', Pipeline([\n",
    "            ('selector', NumberSelector(key='com')),\n",
    "            ('standard', MaxAbsScaler())\n",
    "        ]))\n",
    "    ])),\n",
    "    ('classifier', KNeighborsClassifier(n_neighbors=1, weights='uniform'))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18a2e32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score for KNN on experimental dataset:  [25.91656131 37.70835633 47.18579786 53.24384787]\n",
      "Accuracy score for KNN on experimental dataset:  43.988469999241445\n"
     ]
    }
   ],
   "source": [
    "x_tr_exp, x_te_exp, y_tr_exp, y_te_exp = train_test_split(tweets[['message', 'neg', 'neu', 'pos', 'com']], tweets['sentiment'], test_size=0.3)\n",
    "\n",
    "knn_exp.fit(x_tr_exp, y_tr_exp)\n",
    "knn_exp_predict = knn_exp.predict(x_te_exp)\n",
    "\n",
    "print('F1 score for KNN on experimental dataset: ', f1_score(y_te_exp, knn_exp_predict, average=None) * 100)\n",
    "print('Accuracy score for KNN on experimental dataset: ', accuracy_score(y_te_exp, knn_exp_predict) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf26df8c",
   "metadata": {},
   "source": [
    "## Splitting Dataset\n",
    "\n",
    "I have splitted the dataset into Training Set (TR), and Test Set (TE). Respectively in 85%, and 15%. <br/>\n",
    "[Later](#comparison-of-classification-algorithms), from the Training Set will be extracted the Validation Set (VA)(15%).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d89b8375",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set Length: 37351\n",
      "Test Set Length: 6592\n"
     ]
    }
   ],
   "source": [
    "np.random.seed()\n",
    "\n",
    "x_tr, x_te, y_tr, y_te = train_test_split(tweets['message'], tweets['sentiment'], test_size=0.15)\n",
    "\n",
    "print('Training Set Length:', len(x_tr))\n",
    "print('Test Set Length:', len(x_te))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e78ed75",
   "metadata": {},
   "source": [
    "## Comparison of Classification Algorithms\n",
    "\n",
    "I have chosen five algorithms: ``K-Nearest Neighbors``, ``Multinomial Naive-Bayes``, ``Logistic Regression``, ``SVM``, and ``SGD-Classifier``. <br/>\n",
    "``K-Nearest Neighbors`` and ``SVM`` are an easy geometric approach and a more complex one to classification. <br/>\n",
    "``Multinomial Naive-Bayes`` and ``Logistic Regression`` are an easy probabilistic approach and a more complex one to classification. <br/>\n",
    "``SGD-Classifier`` implements linear algorithms (both geometric and probabilistic) and optimizes them with SGD (Stochastic Gradient Descent).\n",
    "\n",
    "I have made five different pipelines: one for each algorithm. <br/>\n",
    "In each Pipeline I have setted three main steps:\n",
    "- _vect_ &#8594; **CountVectorizer** &#8594; converts a collection of documents (tweets) to a matrix of token counts (Bag-of-Words). It takes this list of parameters:\n",
    "    - *stop_words* &#187; Removes some very common words (based on English vocabulary).\n",
    "    - *strip_accents* &#187; Removes accents and any other not supported characters (for instance, Arabic symbols are not supported in ASCII).\n",
    "    - *lowercase* &#187; Converts all characters to lowercase.\n",
    "    - *max_df* &#187; When building the vocabulary ignores terms that have a document frequency strictly higher than the given threshold.\n",
    "- _tfidf_ &#8594; **TfidfTransformer** &#8594; transforms a count matrix to a normalized tf-idf representation (common term weighting scheme in information retrieval). It takes this list of parameters:\n",
    "    - *use_idf* &#187; Enable inverse-document-frequency reweighting. If False, idf(t) = 1.\n",
    "- _clf_ &#8594; this is the algorithm used for classification.\n",
    "\n",
    "For each pipeline, for each step, there are some parameters which have been setted thanks to experience with ``GridSearchCV``. <br/>\n",
    "*GridSearchCV* uses **Cross-Validation**. But it has been designed to work on limited dataset (and it is not our case). This just slows us down. <br/>\n",
    "To solve this problem, I have setted the parameter *'cv'* using **ShuffleSplit()**, which gets the **Validation Set (15%)** from the TR, and lets the process of Cross-validation repeat just once (instead of 5 times) optimizing the execution time. \n",
    "\n",
    "**N.B.** Before running these codes, I suggest you to set in each _GridSearchCV_ the parameter *n_jobs* with the number of your cores - 2. <br/>\n",
    "**N.B.** Before running these codes, assume that the entire run will last **about 5 hours** instead of **25 hours** (without ShuffleSplit)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f2ca19",
   "metadata": {},
   "source": [
    "#### **K-Nearest Neighbors Algorithm**\n",
    "\n",
    "This algorithm works by finding the distances between a query point and all the examples in the data, selecting the specified number examples (K) closest to the query, then votes for the most frequent label.\n",
    "\n",
    "<img src=\"https://antonioscardace.altervista.org/smm/knn.png\" style=\"height: 300px; margin-top: 10px;\"/>\n",
    "\n",
    "In this case, the classifier takes two parameters:\n",
    "- *n_neighbors* &#187; Number of closest examples to consider during classification. It is usually called *K parameter*.\n",
    "- *weights* &#187; Weight function used in prediction.\n",
    "    - *'uniform'* : uniform weights. All points in each neighborhood are weighted equally.\n",
    "    - *'distance'* : weight points by the inverse of their distance. In this case, closer neighbors will have a greater influence than neighbors which are further away. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c3f5da9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use Stop-Words-Removal:  None\n",
      "Remove Accents:  None\n",
      "Lowercase:  True\n",
      "Max-df:  0.75\n",
      "Use TF-IDF:  False\n",
      "Best K parameter:  1\n",
      "Weight function used in prediction:  uniform\n"
     ]
    }
   ],
   "source": [
    "knn_va_classifier = Pipeline([\n",
    "    ('vect', CountVectorizer(preprocessor=preProc)),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', KNeighborsClassifier())\n",
    "])\n",
    "\n",
    "params = {\n",
    "    'vect__stop_words': (None, 'english'),\n",
    "    'vect__strip_accents': (None, 'unicode'),\n",
    "    'vect__lowercase': [True, False],\n",
    "    'vect__max_df': [0.125, 0.25, 0.375, 0.5, 0.75, 0.875, 1.0],\n",
    "    'tfidf__use_idf': (True, False),\n",
    "    'clf__n_neighbors': np.arange(1, 8),\n",
    "    'clf__weights': ['uniform', 'distance'],\n",
    "}\n",
    "knn_gs = GridSearchCV(knn_va_classifier, params, cv=ShuffleSplit(test_size=0.1765, n_splits=1), n_jobs=4)\n",
    "\n",
    "knn_gs.fit(x_tr, y_tr)\n",
    "\n",
    "print('Use Stop-Words-Removal: ', knn_gs.best_params_['vect__stop_words'])\n",
    "print('Remove Accents: ', knn_gs.best_params_['vect__strip_accents'])\n",
    "print('Lowercase: ', knn_gs.best_params_['vect__lowercase'])\n",
    "print('Max-df: ', knn_gs.best_params_['vect__max_df'])\n",
    "print('Use TF-IDF: ', knn_gs.best_params_['tfidf__use_idf'])\n",
    "print('Best K parameter: ', knn_gs.best_params_['clf__n_neighbors'])\n",
    "print('Weight function used in prediction: ', knn_gs.best_params_['clf__weights'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ed3d36bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score for KNN on TE:  [28.16901408 40.31907179 59.06111603 62.66666667]\n",
      "Accuracy score for KNN on TE:  51.88106796116505\n"
     ]
    }
   ],
   "source": [
    "knn_classifier = Pipeline([\n",
    "    ('vect', CountVectorizer(\n",
    "        stop_words=knn_gs.best_params_['vect__stop_words'],\n",
    "        strip_accents=knn_gs.best_params_['vect__strip_accents'],\n",
    "        lowercase=knn_gs.best_params_['vect__lowercase'],\n",
    "        max_df=knn_gs.best_params_['vect__max_df'],\n",
    "        preprocessor=preProc,\n",
    "    )),\n",
    "    ('tfidf', TfidfTransformer(use_idf=knn_gs.best_params_['tfidf__use_idf'])),\n",
    "    ('clf', KNeighborsClassifier(n_neighbors=knn_gs.best_params_['clf__n_neighbors'], weights=knn_gs.best_params_['clf__weights']))\n",
    "])\n",
    "\n",
    "knn_classifier.fit(x_tr, y_tr)\n",
    "knn_predict = knn_classifier.predict(x_te)\n",
    "\n",
    "print('F1 score for KNN on TE: ', f1_score(y_te, knn_predict, average=None) * 100)\n",
    "print('Accuracy score for KNN on TE: ', accuracy_score(y_te, knn_predict) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66ac005",
   "metadata": {},
   "source": [
    "#### **Multinomial Naive-Bayes Algorithm**\n",
    "\n",
    "This algorithm is a probabilistic approach to classification. It is based on applying Bayes theorem with strong (naive) independence assumptions between the features.\n",
    "\n",
    "In this case, the classifier takes zero parameters: &#8709;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f48247c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use Stop-Words-Removal:  english\n",
      "Remove Accents:  None\n",
      "Lowercase:  True\n",
      "Max-df:  0.125\n",
      "Use TF-IDF:  True\n"
     ]
    }
   ],
   "source": [
    "nb_va_classifier = Pipeline([\n",
    "    ('vect', CountVectorizer(preprocessor=preProc)),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', MultinomialNB())\n",
    "])\n",
    "\n",
    "params = {\n",
    "    'vect__stop_words': (None, 'english'),\n",
    "    'vect__strip_accents': (None, 'unicode'),\n",
    "    'vect__lowercase': [True, False],\n",
    "    'vect__max_df': [0.125, 0.25, 0.375, 0.5, 0.75, 0.875, 1.0],\n",
    "    'tfidf__use_idf': (True, False)\n",
    "}\n",
    "nb_gs = GridSearchCV(nb_va_classifier, params, cv=ShuffleSplit(test_size=0.1765, n_splits=1), n_jobs=4)\n",
    "\n",
    "nb_gs.fit(x_tr, y_tr)\n",
    "\n",
    "print('Use Stop-Words-Removal: ', nb_gs.best_params_['vect__stop_words'])\n",
    "print('Remove Accents: ', nb_gs.best_params_['vect__strip_accents'])\n",
    "print('Lowercase: ', nb_gs.best_params_['vect__lowercase'])\n",
    "print('Max-df: ', nb_gs.best_params_['vect__max_df'])\n",
    "print('Use TF-IDF: ', nb_gs.best_params_['tfidf__use_idf'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c578b049",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score for MultinomialNB on TE:  [ 9.50819672 16.50641026 74.90523124 59.43621596]\n",
      "Accuracy score for MultinomialNB on TE:  63.89563106796117\n"
     ]
    }
   ],
   "source": [
    "nb_classifier = Pipeline([\n",
    "    ('vect', CountVectorizer(\n",
    "        stop_words=nb_gs.best_params_['vect__stop_words'],\n",
    "        strip_accents=nb_gs.best_params_['vect__strip_accents'],\n",
    "        lowercase=nb_gs.best_params_['vect__lowercase'],\n",
    "        max_df=nb_gs.best_params_['vect__max_df'],\n",
    "        preprocessor=preProc\n",
    "    )),\n",
    "    ('tfidf', TfidfTransformer(use_idf=nb_gs.best_params_['tfidf__use_idf'])),\n",
    "    ('clf', MultinomialNB())\n",
    "])\n",
    "\n",
    "nb_classifier.fit(x_tr, y_tr)\n",
    "nb_predict = nb_classifier.predict(x_te)\n",
    "\n",
    "print('F1 score for MultinomialNB on TE: ', f1_score(y_te, nb_predict, average=None) * 100)\n",
    "print('Accuracy score for MultinomialNB on TE: ', accuracy_score(y_te, nb_predict) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56553ba7",
   "metadata": {},
   "source": [
    "#### **Logistic Regression Algorithm**\n",
    "\n",
    "This algorithm is named for the function used at the core of the method, the logistic function. It’s an S-shaped curve that can take any real number and map it into a value (probability) between 0 and 1, but never exactly at those limits.\n",
    "\n",
    "<img src=\"https://antonioscardace.altervista.org/smm/logistic_regression.png\" style=\"height: 300px; margin-top: 10px;\"/>\n",
    "\n",
    "In this case, the classifier takes two parameters:\n",
    "- *max_iter* &#187; Maximum number of iterations taken for the solvers to converge.\n",
    "- *solver* &#187; Algorithm to use in the optimization problem. It depends on dataset size, and on the number of possible output classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f114530e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use Stop-Words-Removal:  None\n",
      "Remove Accents:  None\n",
      "Lowercase:  False\n",
      "Max-df:  0.25\n",
      "Use TF-IDF:  True\n",
      "Maximum number of iterations taken to converge:  600\n",
      "Algorithm to use in the optimization problem:  saga\n"
     ]
    }
   ],
   "source": [
    "lr_va_classifier = Pipeline([\n",
    "    ('vect', CountVectorizer(preprocessor=preProc)),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', LogisticRegression())\n",
    "])\n",
    "\n",
    "params = {\n",
    "    'vect__stop_words': (None, 'english'),\n",
    "    'vect__strip_accents': (None, 'unicode'),\n",
    "    'vect__lowercase': [True, False],\n",
    "    'vect__max_df': [0.125, 0.25, 0.375, 0.5, 0.75, 0.875, 1.0],\n",
    "    'tfidf__use_idf': (True, False),\n",
    "    'clf__max_iter': (600, 700, 800),\n",
    "    'clf__solver': ['liblinear', 'sag', 'saga', 'lbfgs']\n",
    "}\n",
    "lr_gs = GridSearchCV(lr_va_classifier, params, cv=ShuffleSplit(test_size=0.1765, n_splits=1), n_jobs=4)\n",
    "\n",
    "lr_gs.fit(x_tr, y_tr)\n",
    "\n",
    "print('Use Stop-Words-Removal: ', lr_gs.best_params_['vect__stop_words'])\n",
    "print('Remove Accents: ', lr_gs.best_params_['vect__strip_accents'])\n",
    "print('Lowercase: ', lr_gs.best_params_['vect__lowercase'])\n",
    "print('Max-df: ', lr_gs.best_params_['vect__max_df'])\n",
    "print('Use TF-IDF: ', lr_gs.best_params_['tfidf__use_idf'])\n",
    "print('Maximum number of iterations taken to converge: ', lr_gs.best_params_['clf__max_iter'])\n",
    "print('Algorithm to use in the optimization problem: ', lr_gs.best_params_['clf__solver'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b04c75a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score for LogisticRegressor on TE:  [47.80600462 49.12652197 80.04125838 73.84960718]\n",
      "Accuracy score for LogisticRegressor on TE:  72.23907766990291\n"
     ]
    }
   ],
   "source": [
    "lr_classifier = Pipeline([\n",
    "    ('vect', CountVectorizer(\n",
    "        stop_words=lr_gs.best_params_['vect__stop_words'],\n",
    "        strip_accents=lr_gs.best_params_['vect__strip_accents'],\n",
    "        lowercase=lr_gs.best_params_['vect__lowercase'],\n",
    "        max_df=lr_gs.best_params_['vect__max_df'],\n",
    "        preprocessor=preProc\n",
    "    )),\n",
    "    ('tfidf', TfidfTransformer(use_idf=lr_gs.best_params_['tfidf__use_idf'])),\n",
    "    ('clf', LogisticRegression(max_iter=lr_gs.best_params_['clf__max_iter'], solver=lr_gs.best_params_['clf__solver']))\n",
    "])\n",
    "\n",
    "lr_classifier.fit(x_tr, y_tr)\n",
    "lr_predict = lr_classifier.predict(x_te)\n",
    "\n",
    "print('F1 score for LogisticRegressor on TE: ', f1_score(y_te, lr_predict, average=None) * 100)\n",
    "print('Accuracy score for LogisticRegressor on TE: ', accuracy_score(y_te, lr_predict) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c15b64",
   "metadata": {},
   "source": [
    "#### **Support-Vector-Machine (SVM) Algorithm**\n",
    "\n",
    "The objective of the Support Vector Machine algorithm is to find a hyperplane in an N-dimensional space (N = number of features) that distinctly classifies the data points. <br/>\n",
    "We just want to find an hyperplane which has the maximum distance between data points of both classes. Maximizing this distance provides some reinforcement so that future data points can be classified with more confidence.\n",
    "\n",
    "<img src=\"https://antonioscardace.altervista.org/smm/kernel_svm.png\" style=\"height: 250px; margin-top: 10px;\"/> <br/>\n",
    "<img src=\"https://antonioscardace.altervista.org/smm/svm_example.png\" style=\"height: 250px; margin-left:15px; margin-top: 10px;\"/>\n",
    "\n",
    "In this case, the classifier takes one parameter:\n",
    "- *kernel* &#187; Specifies the kernel type to be used in the algorithm which return a \"best fit\" hyperplane which divides (categorizes) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fffec322",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use Stop-Words-Removal:  None\n",
      "Remove Accents:  None\n",
      "Lowercase:  True\n",
      "Max-df:  0.375\n",
      "Use TF-IDF:  True\n",
      "Kernel type used:  linear\n"
     ]
    }
   ],
   "source": [
    "svm_va_classifier = Pipeline([\n",
    "    ('vect', CountVectorizer(preprocessor=preProc)),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', SVC())\n",
    "])\n",
    "\n",
    "params = {\n",
    "    'vect__stop_words': (None, 'english'),\n",
    "    'vect__strip_accents': (None, 'unicode'),\n",
    "    'vect__lowercase': [True, False],\n",
    "    'vect__max_df': [0.125, 0.25, 0.375, 0.5, 0.75, 0.875, 1.0],\n",
    "    'tfidf__use_idf': (True, False),\n",
    "    'clf__kernel': ('linear', 'rbf')\n",
    "}\n",
    "svm_gs = GridSearchCV(svm_va_classifier, params, cv=ShuffleSplit(test_size=0.1765, n_splits=1), n_jobs=4)\n",
    "\n",
    "svm_gs.fit(x_tr, y_tr)\n",
    "\n",
    "print('Use Stop-Words-Removal: ', svm_gs.best_params_['vect__stop_words'])\n",
    "print('Remove Accents: ', svm_gs.best_params_['vect__strip_accents'])\n",
    "print('Lowercase: ', svm_gs.best_params_['vect__lowercase'])\n",
    "print('Max-df: ', svm_gs.best_params_['vect__max_df'])\n",
    "print('Use TF-IDF: ', svm_gs.best_params_['tfidf__use_idf'])\n",
    "print('Kernel type used: ', svm_gs.best_params_['clf__kernel'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9b4afd34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score for SVM on TE:  [55.76923077 51.55374427 80.55811505 75.29761905]\n",
      "Accuracy score for SVM on TE:  73.40716019417476\n"
     ]
    }
   ],
   "source": [
    "svm_classifier = Pipeline([\n",
    "    ('vect', CountVectorizer(\n",
    "        stop_words=svm_gs.best_params_['vect__stop_words'],\n",
    "        strip_accents=svm_gs.best_params_['vect__strip_accents'],\n",
    "        lowercase=svm_gs.best_params_['vect__lowercase'],\n",
    "        max_df=svm_gs.best_params_['vect__max_df'],\n",
    "        preprocessor=preProc\n",
    "    )),\n",
    "    ('tfidf', TfidfTransformer(use_idf=svm_gs.best_params_['tfidf__use_idf'])),\n",
    "    ('clf', SVC(kernel=svm_gs.best_params_['clf__kernel']))\n",
    "])\n",
    "\n",
    "svm_classifier.fit(x_tr, y_tr)\n",
    "svm_predict = svm_classifier.predict(x_te)\n",
    "\n",
    "print('F1 score for SVM on TE: ', f1_score(y_te, svm_predict, average=None) * 100)\n",
    "print('Accuracy score for SVM on TE: ', accuracy_score(y_te, svm_predict) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6909714",
   "metadata": {},
   "source": [
    "#### **SGD (Stochastic Gradient Descent)**\n",
    "\n",
    "It uses linear classifiers (such as SVM and Logistic Regression) optimized by SGD (Stochastic Gradient Descent)\n",
    "\n",
    "In this case, the classifier takes zero parameters: &#8709;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "18c685ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use Stop-Words-Removal:  None\n",
      "Remove Accents:  None\n",
      "Lowercase:  True\n",
      "Max-df:  0.375\n",
      "Use TF-IDF:  True\n"
     ]
    }
   ],
   "source": [
    "sgd_va_classifier = Pipeline([\n",
    "    ('vect', CountVectorizer(preprocessor=preProc)),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', SGDClassifier())\n",
    "])\n",
    "\n",
    "params = {\n",
    "    'vect__stop_words': (None, 'english'),\n",
    "    'vect__strip_accents': (None, 'unicode'),\n",
    "    'vect__lowercase': [True, False],\n",
    "    'vect__max_df': [0.125, 0.25, 0.375, 0.5, 0.75, 0.875, 1.0],\n",
    "    'tfidf__use_idf': (True, False)\n",
    "}\n",
    "sgd_gs = GridSearchCV(sgd_va_classifier, params, cv=ShuffleSplit(test_size=0.1765, n_splits=1), n_jobs=4)\n",
    "\n",
    "sgd_gs.fit(x_tr, y_tr)\n",
    "\n",
    "print('Use Stop-Words-Removal: ', sgd_gs.best_params_['vect__stop_words'])\n",
    "print('Remove Accents: ', sgd_gs.best_params_['vect__strip_accents'])\n",
    "print('Lowercase: ', sgd_gs.best_params_['vect__lowercase'])\n",
    "print('Max-df: ', sgd_gs.best_params_['vect__max_df'])\n",
    "print('Use TF-IDF: ', sgd_gs.best_params_['tfidf__use_idf'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ca187f15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score for SGD on TE:  [41.26582278 40.95778198 79.04515811 71.71641791]\n",
      "Accuracy score for SGD on TE:  70.70691747572816\n"
     ]
    }
   ],
   "source": [
    "sgd_classifier = Pipeline([\n",
    "    ('vect', CountVectorizer(\n",
    "        stop_words=sgd_gs.best_params_['vect__stop_words'],\n",
    "        strip_accents=sgd_gs.best_params_['vect__strip_accents'],\n",
    "        lowercase=sgd_gs.best_params_['vect__lowercase'],\n",
    "        max_df=sgd_gs.best_params_['vect__max_df'],\n",
    "        preprocessor=preProc\n",
    "    )),\n",
    "    ('tfidf', TfidfTransformer(use_idf=sgd_gs.best_params_['tfidf__use_idf'])),\n",
    "    ('clf', SGDClassifier())\n",
    "])\n",
    "\n",
    "sgd_classifier.fit(x_tr, y_tr)\n",
    "sgd_predict = sgd_classifier.predict(x_te)\n",
    "\n",
    "print('F1 score for SGD on TE: ', f1_score(y_te, sgd_predict, average=None) * 100)\n",
    "print('Accuracy score for SGD on TE: ', accuracy_score(y_te, sgd_predict) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab22f151",
   "metadata": {},
   "source": [
    "## Real World Application\n",
    "\n",
    "As we have just seen, among these five algorithms, the best is the **SVM**. We will use it in a real-world application. \n",
    "\n",
    "In the real world, there are probably a lot of applications for this project, but I have been interested by one in particular: <br/>\n",
    "Given the tweets of some Twitter Verified Accounts (VIP such as politicians or actors) we have to be able to understand how much these people care about climate change in their Twitter page. <br/>\n",
    "We will make tests on some US Verified Twitter profiles owned by well known activists (such as **Greta Thunberg** or **Leonardo Di Caprio**). <br/>\n",
    "In time, with an appropriate dataset, may be maked analytics on how many VIPs are interested (and how) on climate change for each main job category (such as politics, cinema, football)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "14135c54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_cb5e6\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_cb5e6_level0_col0\" class=\"col_heading level0 col0\" >message</th>\n",
       "      <th id=\"T_cb5e6_level0_col1\" class=\"col_heading level0 col1\" >predict</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_cb5e6_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_cb5e6_row0_col0\" class=\"data row0 col0\" >RT @Fridays4FutureU: Today #FridaysForFuture Uganda activists have again protested against the East African Crude Oil Pipeline at </td>\n",
       "      <td id=\"T_cb5e6_row0_col1\" class=\"data row0 col1\" >1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_cb5e6_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_cb5e6_row1_col0\" class=\"data row1 col0\" >RT @omaer_alam: Today I participated in the weekly strike against climate change from the bridge of Turag river in Dhaka district </td>\n",
       "      <td id=\"T_cb5e6_row1_col1\" class=\"data row1 col1\" >1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_cb5e6_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_cb5e6_row2_col0\" class=\"data row2 col0\" >RT @Joshomonukk: Keep 1.5 Alive.\n",
       "#FridaysForFuture @Riseupmovt https://t.co/tK8t98dvOo</td>\n",
       "      <td id=\"T_cb5e6_row2_col1\" class=\"data row2 col1\" >1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_cb5e6_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_cb5e6_row3_col0\" class=\"data row3 col0\" >RT @fff_tui: Save life on earth\n",
       "\n",
       "Start with a budget that funds #ClimateActionNow \n",
       "\n",
       "Standing outside #NewZealand Minister of finan</td>\n",
       "      <td id=\"T_cb5e6_row3_col1\" class=\"data row3 col1\" >1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_cb5e6_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_cb5e6_row4_col0\" class=\"data row4 col0\" >RT @EKOenergy_: We're in front the Finnish Parliament, asking for #ClimateAction and #sustainable #RenewableEnergy! \n",
       "\n",
       "#fridaysforf</td>\n",
       "      <td id=\"T_cb5e6_row4_col1\" class=\"data row4 col1\" >1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_cb5e6_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_cb5e6_row5_col0\" class=\"data row5 col0\" >RT @auber_fichess: Week 41 #ClimateStrike in #Angola\n",
       "In Angola the politicians treat the law with a pistol and the 27th of May is </td>\n",
       "      <td id=\"T_cb5e6_row5_col1\" class=\"data row5 col1\" >1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_cb5e6_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_cb5e6_row6_col0\" class=\"data row6 col0\" >RT @FFF_Sweden: Med stor glädje meddelar vi om ett samarbete mellan FFF Sverige och Sáminuorra - vi ska bygga en gruva på Östermal</td>\n",
       "      <td id=\"T_cb5e6_row6_col1\" class=\"data row6 col1\" >0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_cb5e6_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "      <td id=\"T_cb5e6_row7_col0\" class=\"data row7 col0\" >School strike week 189. We recently received an irresistible offer from fossil fuel lobbyists, so we have now changed our narrativ</td>\n",
       "      <td id=\"T_cb5e6_row7_col1\" class=\"data row7 col1\" >1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_cb5e6_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "      <td id=\"T_cb5e6_row8_col0\" class=\"data row8 col0\" >RT @stopEACOP: Last week we left the comfort of our homes and travelled to the lion's den. We were at @totalenergies headquarters </td>\n",
       "      <td id=\"T_cb5e6_row8_col1\" class=\"data row8 col1\" >1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_cb5e6_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "      <td id=\"T_cb5e6_row9_col0\" class=\"data row9 col0\" >Here is the full list of contributors: https://t.co/eNnvrC4miN</td>\n",
       "      <td id=\"T_cb5e6_row9_col1\" class=\"data row9 col1\" >1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_cb5e6_level0_row10\" class=\"row_heading level0 row10\" >10</th>\n",
       "      <td id=\"T_cb5e6_row10_col0\" class=\"data row10 col0\" >I’ve invited over 100 leading voices from around the world - scientists, experts, activists and authors to create a book that cove</td>\n",
       "      <td id=\"T_cb5e6_row10_col1\" class=\"data row10 col1\" >1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_cb5e6_level0_row11\" class=\"row_heading level0 row11\" >11</th>\n",
       "      <td id=\"T_cb5e6_row11_col0\" class=\"data row11 col0\" >RT @tveitdal: Satellite images reveal lake with water supply for 2 million people in Chile has dried up\n",
       "https://t.co/Lr1cuk4xru\n",
       "Ne</td>\n",
       "      <td id=\"T_cb5e6_row11_col1\" class=\"data row11 col1\" >2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_cb5e6_level0_row12\" class=\"row_heading level0 row12\" >12</th>\n",
       "      <td id=\"T_cb5e6_row12_col0\" class=\"data row12 col0\" >RT @sullyfoto: Lake Powell is shrinking. These photos were taken 9 months apart (Top photo 6/23/21 - Bottom photo 3/27/22) at Lake</td>\n",
       "      <td id=\"T_cb5e6_row12_col1\" class=\"data row12 col1\" >1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_cb5e6_level0_row13\" class=\"row_heading level0 row13\" >13</th>\n",
       "      <td id=\"T_cb5e6_row13_col0\" class=\"data row13 col0\" >RT @SEIclimate: The Morrison government plans to cut #climate spending by 35% annually over four years, if it wins the election. A</td>\n",
       "      <td id=\"T_cb5e6_row13_col1\" class=\"data row13 col1\" >1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_cb5e6_level0_row14\" class=\"row_heading level0 row14\" >14</th>\n",
       "      <td id=\"T_cb5e6_row14_col0\" class=\"data row14 col0\" >RT @AnandWrites: Drop everything and read this essential take.\n",
       "\n",
       "https://t.co/3zoVYlUGxv https://t.co/yixSAO3LBq</td>\n",
       "      <td id=\"T_cb5e6_row14_col1\" class=\"data row14 col1\" >1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f1e1846b2e0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "greta = AnalyzeVIP('@GretaThunberg')\n",
    "results = greta.makeTable(svm_classifier, 15)\n",
    "\n",
    "results['message'] = results['message'].str[:130]\n",
    "results.style"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44aac22e",
   "metadata": {},
   "source": [
    "As we can see, Greta Thunberg (activist) tweets are almost all about Climate Change and she supports the belief of man-made climate change. <br/>\n",
    "We can note that 11th tweet is a news, and 6th tweet is in swedish and our classifier is not able to analyze this language, so it classied it as Neutral. <br/>\n",
    "Last tweet on the contrary, is not about climate change, but it has been classified as Pro. It is an error!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a7d09b44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_7dc0b\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_7dc0b_level0_col0\" class=\"col_heading level0 col0\" >message</th>\n",
       "      <th id=\"T_7dc0b_level0_col1\" class=\"col_heading level0 col1\" >predict</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_7dc0b_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_7dc0b_row0_col0\" class=\"data row0 col0\" >It’s time for people to feel good about their purchases and for businesses to meet that challenge. As a Strategic Advisor, I am ex</td>\n",
       "      <td id=\"T_7dc0b_row0_col1\" class=\"data row0 col1\" >1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7dc0b_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_7dc0b_row1_col0\" class=\"data row1 col0\" >@NPR @UN: https://t.co/8SxRVbwIAU</td>\n",
       "      <td id=\"T_7dc0b_row1_col1\" class=\"data row1 col1\" >0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7dc0b_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_7dc0b_row2_col0\" class=\"data row2 col0\" >@newscientist: “Planting trillions of trees won’t replace the 10 million hectares of forest ecosystems lost each year, but documen</td>\n",
       "      <td id=\"T_7dc0b_row2_col1\" class=\"data row2 col1\" >1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7dc0b_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_7dc0b_row3_col0\" class=\"data row3 col0\" >Since October 2020, @NatGeo has documented a pattern of ReconAfrica breaking rules and ignoring environmental and community concer</td>\n",
       "      <td id=\"T_7dc0b_row3_col1\" class=\"data row3 col1\" >1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7dc0b_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_7dc0b_row4_col0\" class=\"data row4 col0\" >Coral scientist @ProfTerryHughes claims a 6th mass bleaching is unfolding across the #GreatBarrierReef. @UNESCO’s World Heritage C</td>\n",
       "      <td id=\"T_7dc0b_row4_col1\" class=\"data row4 col1\" >1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7dc0b_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_7dc0b_row5_col0\" class=\"data row5 col0\" >It’s great to see @TheSolutionsProject be recognized for their amazing work toward resolving the climate crisis. https://t.co/TR13</td>\n",
       "      <td id=\"T_7dc0b_row5_col1\" class=\"data row5 col1\" >1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7dc0b_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_7dc0b_row6_col0\" class=\"data row6 col0\" >Recent @IPCC_CH report tells a sobering truth: Nearly half of humanity is living in the danger zone now.\n",
       "\n",
       "The facts are undeniable</td>\n",
       "      <td id=\"T_7dc0b_row6_col1\" class=\"data row6 col1\" >1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7dc0b_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "      <td id=\"T_7dc0b_row7_col0\" class=\"data row7 col0\" >.@CityNational’s parent company @RBC is violating the rights of indigenous Wet'suwet'en people &amp; bankrolling climate crisis.Jo</td>\n",
       "      <td id=\"T_7dc0b_row7_col1\" class=\"data row7 col1\" >1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7dc0b_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "      <td id=\"T_7dc0b_row8_col0\" class=\"data row8 col0\" >Today, our #JustLookUp coalition kicks off at 12:30pm, at Pershing Square in Los Angeles – be there to join the movement and march</td>\n",
       "      <td id=\"T_7dc0b_row8_col1\" class=\"data row8 col1\" >1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7dc0b_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "      <td id=\"T_7dc0b_row9_col0\" class=\"data row9 col0\" >Wild fish populations are threatened more than ever before. I’m pleased to be an investor in @wildtypefoods, the clear leader in c</td>\n",
       "      <td id=\"T_7dc0b_row9_col1\" class=\"data row9 col1\" >1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7dc0b_level0_row10\" class=\"row_heading level0 row10\" >10</th>\n",
       "      <td id=\"T_7dc0b_row10_col0\" class=\"data row10 col0\" >From protecting biodiversity on its land, to using 100% renewable electricity, @maisontelmont is determined to radically lower its</td>\n",
       "      <td id=\"T_7dc0b_row10_col1\" class=\"data row10 col1\" >1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7dc0b_level0_row11\" class=\"row_heading level0 row11\" >11</th>\n",
       "      <td id=\"T_7dc0b_row11_col0\" class=\"data row11 col0\" >https://t.co/bEXIPZBU8p</td>\n",
       "      <td id=\"T_7dc0b_row11_col1\" class=\"data row11 col1\" >0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7dc0b_level0_row12\" class=\"row_heading level0 row12\" >12</th>\n",
       "      <td id=\"T_7dc0b_row12_col0\" class=\"data row12 col0\" >https://t.co/6ywOrCz4eI</td>\n",
       "      <td id=\"T_7dc0b_row12_col1\" class=\"data row12 col1\" >0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7dc0b_level0_row13\" class=\"row_heading level0 row13\" >13</th>\n",
       "      <td id=\"T_7dc0b_row13_col0\" class=\"data row13 col0\" >https://t.co/29B47AUtpI</td>\n",
       "      <td id=\"T_7dc0b_row13_col1\" class=\"data row13 col1\" >0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7dc0b_level0_row14\" class=\"row_heading level0 row14\" >14</th>\n",
       "      <td id=\"T_7dc0b_row14_col0\" class=\"data row14 col0\" >https://t.co/LnWG4TlaHj</td>\n",
       "      <td id=\"T_7dc0b_row14_col1\" class=\"data row14 col1\" >0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f1e0cec9eb0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "leo = AnalyzeVIP('@LeoDiCaprio')\n",
    "results = leo.makeTable(svm_classifier, 15)\n",
    "\n",
    "results['message'] = results['message'].str[:130]\n",
    "results.style"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9993e656",
   "metadata": {},
   "source": [
    "As we can see, Leonardo Di Caprio (actor and activist) tweets are almost all about Climate Change and he supports the belief of man-made climate change."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1d7601",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "I have tested and compared five classification algorithms. The best is SVM, followed in order by Logistic Regression, SGD, K-Nearest Neighbors, and Multinomial Naive-Bayes. <br/>\n",
    "To better classify text should be used a Neural Network.\n",
    "\n",
    "I got Pros 🆗 and Cons ⛔ in this project due to the basic models used, and the not well-formed dataset (it isn't well proportioned):\n",
    "* 🆗 when tweets are really about climate change, the model works well enough.\n",
    "* 🆗 it is \"quick\" and was really useful for introduce me into Machine Learning world.\n",
    "* 🆗 it is easy to read and has been easy to write.\n",
    "* ⛔ when tweets are not about climate change, the model doesn't works (it base probably its prediction on tweet sentiment without understand really the topic).\n",
    "* ⛔ when tweets contains humorism, the model doesn't work very well.\n",
    "\n",
    "To conclude: I have learned a lot by implementing this project. I have used in pratic a lot of theoric notions studied during my Social Media Management course at university. <br/>\n",
    "Moreover, I have learned that Machine Learning is a big field useful for many real ideas and applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5470dafd",
   "metadata": {},
   "source": [
    "## Bibliography\n",
    "\n",
    "All info about sklearn classes and functions have been taken from: [scikit-learn.org](https://scikit-learn.org/stable/index.html). <br/>\n",
    "All info about Twitter APIs have been taken from: [developer.twitter.com](https://developer.twitter.com/en/docs/twitter-api). <br/>\n",
    "Dataset and idea have been taken from: [kaggle.com/edqian/twitter-climate-change-sentiment-dataset](https://www.kaggle.com/datasets/edqian/twitter-climate-change-sentiment-dataset)."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b7dac55ea79ff990472b3fd73a3a4712c78dbca80cb44a0afa7f2ba3801ae5a9"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
